{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports and set up logging\n",
    "import gensim \n",
    "import logging\n",
    "import glob, os\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# directory containing all source texts for training the model \n",
    "data_dir=\"../corpus\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rom_t.txt = 130885 chars\n",
      "Sonnets_x.txt = 97204 chars\n",
      "Lucrece_x.txt = 86177 chars\n",
      "John_t.txt = 112414 chars\n",
      "R3_h.txt = 156881 chars\n",
      "H5_h.txt = 141819 chars\n",
      "Ant_t.txt = 133668 chars\n",
      "JC_t.txt = 104561 chars\n",
      "WT_c.txt = 134528 chars\n",
      "Tro_c.txt = 142635 chars\n",
      "MerchV_c.txt = 112334 chars\n",
      "MND_c.txt = 88608 chars\n",
      "Ado_c.txt = 111116 chars\n",
      "Pericles_x.txt = 97471 chars\n",
      "R2_h.txt = 120934 chars\n",
      "Mac_t.txt = 91625 chars\n",
      "Lear_t.txt = 140510 chars\n",
      "1H4_h.txt = 130423 chars\n",
      "1H6_h.txt = 116317 chars\n",
      "VenusAdonis_x.txt = 55527 chars\n",
      "LLL_c.txt = 115391 chars\n",
      "Ham_t.txt = 163429 chars\n",
      "H8_h.txt = 128223 chars\n",
      "Tmp_c.txt = 88800 chars\n",
      "Err_c.txt = 76924 chars\n",
      "Wiv_c.txt = 115202 chars\n",
      "TN_c.txt = 104476 chars\n",
      "M4M_c.txt = 116348 chars\n",
      "2H6_h.txt = 135863 chars\n",
      "AYL_c.txt = 114429 chars\n",
      "TGV_c.txt = 91686 chars\n",
      "Shr_c.txt = 111364 chars\n",
      "2H4_h.txt = 141574 chars\n",
      "AWW_c.txt = 121896 chars\n",
      "Cym_t.txt = 147159 chars\n",
      "Cor_t.txt = 146443 chars\n",
      "Tim_t.txt = 98749 chars\n",
      "PhxTur_x.txt = 2072 chars\n",
      "TNK_x.txt = 127691 chars\n",
      "3H6_h.txt = 129551 chars\n",
      "Tit_t.txt = 109892 chars\n",
      "Oth_t.txt = 141395 chars\n"
     ]
    }
   ],
   "source": [
    "os.chdir(data_dir)\n",
    "documents = list()\n",
    "for filename in glob.glob(\"*.txt\"):\n",
    "    filedata = open(filename, 'r').read()\n",
    "    print(filename + \" = \" + str(len(filedata)) + \" chars\")\n",
    "    documents = documents + filedata.split(\".\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Two households , both alike in dignity ( In fair Verona , where we lay our scene ) , From ancient grudge break to new mutiny , Where civil blood makes civil hands unclean \n"
     ]
    }
   ],
   "source": [
    "# Check to see that the first sentence is correct\n",
    "print(documents[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-05-13 16:04:21,530 : INFO : collecting all words and their counts\n",
      "2021-05-13 16:04:21,531 : INFO : PROGRESS: at sentence #0, processed 0 words and 0 word types\n",
      "2021-05-13 16:04:21,922 : INFO : PROGRESS: at sentence #10000, processed 213768 words and 111735 word types\n",
      "2021-05-13 16:04:22,285 : INFO : PROGRESS: at sentence #20000, processed 415146 words and 181236 word types\n",
      "2021-05-13 16:04:22,648 : INFO : PROGRESS: at sentence #30000, processed 615045 words and 244308 word types\n",
      "2021-05-13 16:04:22,981 : INFO : PROGRESS: at sentence #40000, processed 800114 words and 290113 word types\n",
      "2021-05-13 16:04:23,318 : INFO : PROGRESS: at sentence #50000, processed 990140 words and 337035 word types\n",
      "2021-05-13 16:04:23,556 : INFO : collected 365018 word types from a corpus of 1107953 words (unigram + bigrams) and 55651 sentences\n",
      "2021-05-13 16:04:23,557 : INFO : using 365018 counts as vocab in Phrases<0 vocab, min_count=5, threshold=10.0, max_vocab_size=40000000>\n",
      "2021-05-13 16:04:23,557 : INFO : source_vocab length 365018\n",
      "2021-05-13 16:04:27,093 : INFO : Phraser built with 1137 phrasegrams\n",
      "2021-05-13 16:04:27,106 : INFO : collecting all words and their counts\n",
      "2021-05-13 16:04:27,107 : INFO : PROGRESS: at sentence #0, processed 0 words and 0 word types\n",
      "2021-05-13 16:04:28,048 : INFO : PROGRESS: at sentence #10000, processed 187722 words and 110733 word types\n",
      "2021-05-13 16:04:28,941 : INFO : PROGRESS: at sentence #20000, processed 362821 words and 181110 word types\n",
      "2021-05-13 16:04:29,821 : INFO : PROGRESS: at sentence #30000, processed 536390 words and 244973 word types\n",
      "2021-05-13 16:04:30,628 : INFO : PROGRESS: at sentence #40000, processed 694812 words and 291943 word types\n",
      "2021-05-13 16:04:31,439 : INFO : PROGRESS: at sentence #50000, processed 858401 words and 340162 word types\n",
      "2021-05-13 16:04:31,991 : INFO : collected 369042 word types from a corpus of 961298 words (unigram + bigrams) and 55651 sentences\n",
      "2021-05-13 16:04:31,992 : INFO : using 369042 counts as vocab in Phrases<0 vocab, min_count=5, threshold=10.0, max_vocab_size=40000000>\n",
      "2021-05-13 16:04:31,992 : INFO : source_vocab length 369042\n",
      "2021-05-13 16:04:35,661 : INFO : Phraser built with 2603 phrasegrams\n",
      "2021-05-13 16:04:44,369 : INFO : collecting all words and their counts\n",
      "2021-05-13 16:04:44,370 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2021-05-13 16:04:44,413 : INFO : PROGRESS: at sentence #10000, processed 185969 words, keeping 15833 word types\n",
      "2021-05-13 16:04:44,460 : INFO : PROGRESS: at sentence #20000, processed 359151 words, keeping 21909 word types\n",
      "2021-05-13 16:04:44,502 : INFO : PROGRESS: at sentence #30000, processed 530768 words, keeping 27149 word types\n",
      "2021-05-13 16:04:44,545 : INFO : PROGRESS: at sentence #40000, processed 687284 words, keeping 30434 word types\n",
      "2021-05-13 16:04:44,588 : INFO : PROGRESS: at sentence #50000, processed 848968 words, keeping 33414 word types\n",
      "2021-05-13 16:04:44,616 : INFO : collected 34965 word types from a corpus of 950847 raw words and 55651 sentences\n",
      "2021-05-13 16:04:44,617 : INFO : Loading a fresh vocabulary\n",
      "2021-05-13 16:04:44,842 : INFO : effective_min_count=1 retains 34965 unique words (100% of original 34965, drops 0)\n",
      "2021-05-13 16:04:44,842 : INFO : effective_min_count=1 leaves 950847 word corpus (100% of original 950847, drops 0)\n",
      "2021-05-13 16:04:44,955 : INFO : deleting the raw counts dictionary of 34965 items\n",
      "2021-05-13 16:04:44,957 : INFO : sample=0.001 downsamples 46 most-common words\n",
      "2021-05-13 16:04:44,957 : INFO : downsampling leaves estimated 711826 word corpus (74.9% of prior 950847)\n",
      "2021-05-13 16:04:45,060 : INFO : estimated required memory for 34965 words and 200 dimensions: 73426500 bytes\n",
      "2021-05-13 16:04:45,061 : INFO : resetting layer weights\n",
      "2021-05-13 16:04:52,778 : INFO : training model with 20 workers on 34965 vocabulary and 200 features, using sg=1 hs=0 sample=0.001 negative=5 window=5\n",
      "2021-05-13 16:04:53,814 : INFO : EPOCH 1 - PROGRESS: at 73.10% examples, 517758 words/s, in_qsize 26, out_qsize 0\n",
      "2021-05-13 16:04:53,913 : INFO : worker thread finished; awaiting finish of 19 more threads\n",
      "2021-05-13 16:04:53,914 : INFO : worker thread finished; awaiting finish of 18 more threads\n",
      "2021-05-13 16:04:53,924 : INFO : worker thread finished; awaiting finish of 17 more threads\n",
      "2021-05-13 16:04:53,926 : INFO : worker thread finished; awaiting finish of 16 more threads\n",
      "2021-05-13 16:04:53,945 : INFO : worker thread finished; awaiting finish of 15 more threads\n",
      "2021-05-13 16:04:53,950 : INFO : worker thread finished; awaiting finish of 14 more threads\n",
      "2021-05-13 16:04:53,959 : INFO : worker thread finished; awaiting finish of 13 more threads\n",
      "2021-05-13 16:04:53,977 : INFO : worker thread finished; awaiting finish of 12 more threads\n",
      "2021-05-13 16:04:53,986 : INFO : worker thread finished; awaiting finish of 11 more threads\n",
      "2021-05-13 16:04:54,018 : INFO : worker thread finished; awaiting finish of 10 more threads\n",
      "2021-05-13 16:04:54,021 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2021-05-13 16:04:54,026 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2021-05-13 16:04:54,035 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2021-05-13 16:04:54,045 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2021-05-13 16:04:54,046 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2021-05-13 16:04:54,053 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2021-05-13 16:04:54,064 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2021-05-13 16:04:54,065 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-05-13 16:04:54,066 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-05-13 16:04:54,070 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-05-13 16:04:54,071 : INFO : EPOCH - 1 : training on 950847 raw words (711424 effective words) took 1.3s, 561901 effective words/s\n",
      "2021-05-13 16:04:55,087 : INFO : EPOCH 2 - PROGRESS: at 66.81% examples, 477778 words/s, in_qsize 32, out_qsize 0\n",
      "2021-05-13 16:04:55,235 : INFO : worker thread finished; awaiting finish of 19 more threads\n",
      "2021-05-13 16:04:55,241 : INFO : worker thread finished; awaiting finish of 18 more threads\n",
      "2021-05-13 16:04:55,246 : INFO : worker thread finished; awaiting finish of 17 more threads\n",
      "2021-05-13 16:04:55,269 : INFO : worker thread finished; awaiting finish of 16 more threads\n",
      "2021-05-13 16:04:55,291 : INFO : worker thread finished; awaiting finish of 15 more threads\n",
      "2021-05-13 16:04:55,309 : INFO : worker thread finished; awaiting finish of 14 more threads\n",
      "2021-05-13 16:04:55,316 : INFO : worker thread finished; awaiting finish of 13 more threads\n",
      "2021-05-13 16:04:55,335 : INFO : worker thread finished; awaiting finish of 12 more threads\n",
      "2021-05-13 16:04:55,339 : INFO : worker thread finished; awaiting finish of 11 more threads\n",
      "2021-05-13 16:04:55,362 : INFO : worker thread finished; awaiting finish of 10 more threads\n",
      "2021-05-13 16:04:55,367 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2021-05-13 16:04:55,377 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2021-05-13 16:04:55,378 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2021-05-13 16:04:55,391 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2021-05-13 16:04:55,395 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2021-05-13 16:04:55,397 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2021-05-13 16:04:55,399 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2021-05-13 16:04:55,400 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-05-13 16:04:55,402 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-05-13 16:04:55,421 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-05-13 16:04:55,422 : INFO : EPOCH - 2 : training on 950847 raw words (711533 effective words) took 1.3s, 532410 effective words/s\n",
      "2021-05-13 16:04:56,444 : INFO : EPOCH 3 - PROGRESS: at 60.90% examples, 440106 words/s, in_qsize 37, out_qsize 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-05-13 16:04:56,636 : INFO : worker thread finished; awaiting finish of 19 more threads\n",
      "2021-05-13 16:04:56,683 : INFO : worker thread finished; awaiting finish of 18 more threads\n",
      "2021-05-13 16:04:56,700 : INFO : worker thread finished; awaiting finish of 17 more threads\n",
      "2021-05-13 16:04:56,714 : INFO : worker thread finished; awaiting finish of 16 more threads\n",
      "2021-05-13 16:04:56,721 : INFO : worker thread finished; awaiting finish of 15 more threads\n",
      "2021-05-13 16:04:56,735 : INFO : worker thread finished; awaiting finish of 14 more threads\n",
      "2021-05-13 16:04:56,760 : INFO : worker thread finished; awaiting finish of 13 more threads\n",
      "2021-05-13 16:04:56,764 : INFO : worker thread finished; awaiting finish of 12 more threads\n",
      "2021-05-13 16:04:56,781 : INFO : worker thread finished; awaiting finish of 11 more threads\n",
      "2021-05-13 16:04:56,806 : INFO : worker thread finished; awaiting finish of 10 more threads\n",
      "2021-05-13 16:04:56,814 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2021-05-13 16:04:56,825 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2021-05-13 16:04:56,831 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2021-05-13 16:04:56,836 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2021-05-13 16:04:56,838 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2021-05-13 16:04:56,839 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2021-05-13 16:04:56,840 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2021-05-13 16:04:56,841 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-05-13 16:04:56,844 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-05-13 16:04:56,855 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-05-13 16:04:56,857 : INFO : EPOCH - 3 : training on 950847 raw words (711935 effective words) took 1.4s, 502596 effective words/s\n",
      "2021-05-13 16:04:57,892 : INFO : EPOCH 4 - PROGRESS: at 55.27% examples, 398113 words/s, in_qsize 38, out_qsize 1\n",
      "2021-05-13 16:04:58,135 : INFO : worker thread finished; awaiting finish of 19 more threads\n",
      "2021-05-13 16:04:58,153 : INFO : worker thread finished; awaiting finish of 18 more threads\n",
      "2021-05-13 16:04:58,156 : INFO : worker thread finished; awaiting finish of 17 more threads\n",
      "2021-05-13 16:04:58,159 : INFO : worker thread finished; awaiting finish of 16 more threads\n",
      "2021-05-13 16:04:58,181 : INFO : worker thread finished; awaiting finish of 15 more threads\n",
      "2021-05-13 16:04:58,183 : INFO : worker thread finished; awaiting finish of 14 more threads\n",
      "2021-05-13 16:04:58,185 : INFO : worker thread finished; awaiting finish of 13 more threads\n",
      "2021-05-13 16:04:58,200 : INFO : worker thread finished; awaiting finish of 12 more threads\n",
      "2021-05-13 16:04:58,211 : INFO : worker thread finished; awaiting finish of 11 more threads\n",
      "2021-05-13 16:04:58,215 : INFO : worker thread finished; awaiting finish of 10 more threads\n",
      "2021-05-13 16:04:58,235 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2021-05-13 16:04:58,242 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2021-05-13 16:04:58,255 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2021-05-13 16:04:58,258 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2021-05-13 16:04:58,262 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2021-05-13 16:04:58,274 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2021-05-13 16:04:58,275 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2021-05-13 16:04:58,282 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-05-13 16:04:58,283 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-05-13 16:04:58,288 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-05-13 16:04:58,290 : INFO : EPOCH - 4 : training on 950847 raw words (712077 effective words) took 1.4s, 503048 effective words/s\n",
      "2021-05-13 16:04:59,310 : INFO : EPOCH 5 - PROGRESS: at 51.75% examples, 380355 words/s, in_qsize 39, out_qsize 0\n",
      "2021-05-13 16:04:59,696 : INFO : worker thread finished; awaiting finish of 19 more threads\n",
      "2021-05-13 16:04:59,698 : INFO : worker thread finished; awaiting finish of 18 more threads\n",
      "2021-05-13 16:04:59,699 : INFO : worker thread finished; awaiting finish of 17 more threads\n",
      "2021-05-13 16:04:59,700 : INFO : worker thread finished; awaiting finish of 16 more threads\n",
      "2021-05-13 16:04:59,708 : INFO : worker thread finished; awaiting finish of 15 more threads\n",
      "2021-05-13 16:04:59,723 : INFO : worker thread finished; awaiting finish of 14 more threads\n",
      "2021-05-13 16:04:59,751 : INFO : worker thread finished; awaiting finish of 13 more threads\n",
      "2021-05-13 16:04:59,753 : INFO : worker thread finished; awaiting finish of 12 more threads\n",
      "2021-05-13 16:04:59,756 : INFO : worker thread finished; awaiting finish of 11 more threads\n",
      "2021-05-13 16:04:59,757 : INFO : worker thread finished; awaiting finish of 10 more threads\n",
      "2021-05-13 16:04:59,758 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2021-05-13 16:04:59,759 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2021-05-13 16:04:59,760 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2021-05-13 16:04:59,761 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2021-05-13 16:04:59,786 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2021-05-13 16:04:59,793 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2021-05-13 16:04:59,799 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2021-05-13 16:04:59,802 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-05-13 16:04:59,803 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-05-13 16:04:59,814 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-05-13 16:04:59,815 : INFO : EPOCH - 5 : training on 950847 raw words (711905 effective words) took 1.5s, 471588 effective words/s\n",
      "2021-05-13 16:04:59,815 : INFO : training on a 4754235 raw words (3558874 effective words) took 7.0s, 505764 effective words/s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Two', 'households', ',', 'both', 'alike', 'in', 'dignity', '(', 'In', 'fair']\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import word2vec, Phrases\n",
    "from gensim.models.phrases import Phraser\n",
    "\n",
    "#documents = [\"the mayor of new york was there\", \"human computer interaction and machine learning has now become a trending research area\",\"human computer interaction is interesting\",\"human computer interaction is a pretty interesting subject\", \"human computer interaction is a great and new subject\", \"machine learning can be useful sometimes\",\"new york mayor was present\", \"I love machine learning because it is a new subject area\", \"human computer interaction helps people to get user friendly applications\"]\n",
    "\n",
    "sentence_stream = [doc.split(\" \") for doc in documents]\n",
    "\n",
    "trigram_sentences_project = []\n",
    "\n",
    "bigram = Phraser(Phrases(sentence_stream))\n",
    "trigram = Phraser(Phrases(bigram[sentence_stream]))\n",
    "\n",
    "for sent in sentence_stream:\n",
    "    bigrams_ = bigram[sent]\n",
    "    trigrams_ = trigram[bigram[sent]]\n",
    "    trigram_sentences_project.append(trigrams_)\n",
    "\n",
    "# Set values for various parameters\n",
    "num_features = 200    # Word vector dimensionality                      \n",
    "min_word_count = 1    # Minimum word count                        \n",
    "num_workers = 20      # Number of threads to run in parallel\n",
    "context = 5           # Context window size                                                                                    \n",
    "downsampling = 1e-3   # Downsample setting for frequent words\n",
    "skip_grams = 1        # 0 for CBOW, 1 for skip-grams\n",
    "\n",
    "model = word2vec.Word2Vec(trigram_sentences_project, workers=num_workers, \\\n",
    "            size=num_features, min_count = min_word_count, \\\n",
    "            window = context, sample = downsampling, sg = skip_grams)\n",
    "\n",
    "vocab = list(model.wv.vocab.keys())\n",
    "print(vocab[:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34965\n"
     ]
    }
   ],
   "source": [
    "# Print the total number of items in our model's vocabulary\n",
    "print(len(model.wv.vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-05-13 16:04:59,828 : INFO : precomputing L2-norms of word weight vectors\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('Richard', 0.8683132529258728),\n",
       " ('duke', 0.8626935482025146),\n",
       " ('brother', 0.8494663238525391),\n",
       " ('Duke_of_York', 0.8491846323013306),\n",
       " ('heir', 0.8477762937545776),\n",
       " ('prince', 0.8455885052680969),\n",
       " ('Plantagenet', 0.8452500104904175),\n",
       " ('Edward', 0.8438208103179932),\n",
       " ('lord', 0.8423137068748474),\n",
       " ('queen', 0.8402426242828369)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w1 = \"king\"\n",
    "model.wv.most_similar (positive=w1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('sister', 0.9183434247970581),\n",
       " ('Cassio', 0.9092116355895996),\n",
       " ('mistress', 0.8986281156539917),\n",
       " ('niece', 0.8979301452636719),\n",
       " ('coz', 0.8938681483268738),\n",
       " ('husband', 0.8873881697654724),\n",
       " ('hostess', 0.8857955932617188),\n",
       " ('gentlewoman', 0.8824229836463928),\n",
       " ('Iago', 0.8793331980705261),\n",
       " ('girl', 0.874946117401123)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w1 = \"lady\"\n",
    "model.wv.most_similar (positive=w1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('life', 0.8386701345443726),\n",
       " ('revenge', 0.8116271495819092),\n",
       " ('spite', 0.8091097474098206),\n",
       " ('doom', 0.8020980358123779),\n",
       " ('thrice', 0.7985759973526001),\n",
       " ('wealth', 0.797386884689331),\n",
       " ('pain', 0.796976625919342),\n",
       " ('eternal', 0.7952563166618347),\n",
       " ('shame', 0.793511152267456),\n",
       " ('fortune', 0.7901196479797363)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w1 = \"death\"\n",
    "model.wv.most_similar (positive=w1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "queen: 0.8480\n"
     ]
    }
   ],
   "source": [
    "# Check the \"most similar words\", using the default \"cosine similarity\" measure.\n",
    "\n",
    "result = model.wv.most_similar(positive=['woman', 'king'], negative=['man'])\n",
    "\n",
    "most_similar_key, similarity = result[0]  # look at the first match\n",
    "\n",
    "print(f\"{most_similar_key}: {similarity:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "queen: 0.9524\n"
     ]
    }
   ],
   "source": [
    "# Use a different similarity measure: \"cosmul\".\n",
    "result = model.wv.most_similar_cosmul(positive=['woman', 'king'], negative=['man'])\n",
    "most_similar_key, similarity = result[0]  # look at the first match\n",
    "print(f\"{most_similar_key}: {similarity:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('eats', 0.8356186747550964),\n",
       " ('bites', 0.8340573310852051),\n",
       " ('fell', 0.7953795790672302),\n",
       " ('enjoys', 0.793015718460083),\n",
       " ('carries', 0.7922528982162476),\n",
       " ('toe', 0.7900422811508179),\n",
       " ('Looks', 0.7895674109458923),\n",
       " ('pays', 0.7881354689598083),\n",
       " ('tilt-yard', 0.7853092551231384),\n",
       " ('club', 0.7852882146835327)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# analogies -- this example asks:\n",
    "# \"she\" is to \"sings\" as \"he\" is to ...   (analogies are often written like this: \"she:sings::he:?\")\n",
    "model.wv.most_similar(positive=['he','sings'],negative=['she'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('die', 0.00041152595),\n",
       " ('must', 0.0004105362),\n",
       " ('would', 0.00040491138),\n",
       " ('ere', 0.00039016653),\n",
       " ('if', 0.00037956322),\n",
       " ('could', 0.00037884762),\n",
       " ('thee', 0.00037528345),\n",
       " ('never', 0.00037215083),\n",
       " ('live', 0.0003514299),\n",
       " ('should', 0.0003511149),\n",
       " ('shall', 0.00034444567),\n",
       " ('’t', 0.00033022612),\n",
       " ('again', 0.00032597245),\n",
       " ('will', 0.00031302977),\n",
       " ('before', 0.0003000862),\n",
       " ('If', 0.00029950077),\n",
       " ('may', 0.00029627953),\n",
       " ('cannot', 0.00028493916),\n",
       " ('For', 0.0002841173),\n",
       " ('see', 0.00028402795),\n",
       " ('till', 0.00027762493),\n",
       " ('ne’er', 0.00026592638),\n",
       " ('might', 0.00026217752),\n",
       " ('can', 0.00025999063),\n",
       " ('then', 0.00025734946),\n",
       " ('say', 0.00025661412),\n",
       " ('I’ll', 0.0002487981),\n",
       " ('or', 0.00024638235),\n",
       " ('though', 0.0002440708),\n",
       " ('when', 0.00023445417)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict_output_word([\"I\",\"die\",\"again\"], topn=30)  # also the most basic way one could implement text completion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/tharsen/Desktop/Advanced TextViz Workshop 2021/corpus\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tharsen/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:10: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    }
   ],
   "source": [
    "#change these locations to somewhere on your machine\n",
    "print(os.getcwd())\n",
    "tensorsfp = \"../2. Word2Vec/tensorsfp.txt\"\n",
    "metadatafp = \"../2. Word2Vec/metadatafp.txt\"\n",
    "\n",
    "with open( tensorsfp, 'w+') as tensors:\n",
    "    with open( metadatafp, 'w+') as metadata:\n",
    "         for word in model.wv.index2word:\n",
    "                metadata.write(word + '\\n')\n",
    "                vector_row = '\\t'.join(map(str, model[word]))\n",
    "                tensors.write(vector_row + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
